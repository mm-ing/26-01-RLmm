{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7b68d29",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916a9e85",
   "metadata": {},
   "source": [
    "Prompt:\n",
    "Ich bin Dozent für Reinforcement Learning und meine Studenten sind Ingenieure, die das Thema lernen wollen. Bitte erkläre die Methode XXX in einer einfachen strukturierten Art und weise auf die wesentlichen Punkte dieser Methode hin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8dc506b",
   "metadata": {},
   "source": [
    "# Model Based Learing\n",
    "\n",
    "### Grundidee\n",
    "Der Agent lernt nicht nur eine Policy, sondern zusätzlich ein Modell der Umwelt:\n",
    "- Transitionsmodell: Wie verändert sich der Zustand?\n",
    "$$\\hat {s}_{t+1}=f_{\\theta }(s_t,a_t)$$\n",
    "- Rewardmodell: Welcher Reward entsteht?\n",
    "$$\\hat {r}_t=g_{\\theta }(s_t,a_t)$$\n",
    "Mit diesem Modell kann der Agent in Gedanken simulieren, bevor er in der echten Umgebung handelt.\n",
    "Merksatz\n",
    "Model‑Based RL = Lernen + Planen. Der Agent baut sich seine eigene Welt im Kopf.\n",
    "\n",
    "\n",
    "### Nutzen\n",
    "- Weniger reale Interaktionen nötig → hohe Sample‑Effizienz\n",
    "- Planung möglich → ähnlich wie MPC (Model Predictive Control)\n",
    "- Sicherheit → riskante Aktionen können im Modell getestet werden\n",
    "- Generalisation → das Modell kann für viele Policies wiederverwendet werden\n",
    "\n",
    "### Die drei Hauptbausteine\n",
    "(A) Modell lernen\n",
    "Der Agent sammelt Daten und trainiert ein dynamisches Modell:\n",
    "- oft neuronale Netze\n",
    "- manchmal Ensembles (für Unsicherheit)\n",
    "(B) Planung im Modell\n",
    "Der Agent nutzt das Modell, um Aktionen zu simulieren:\n",
    "- Rollouts\n",
    "- Monte‑Carlo‑Planung\n",
    "- Model Predictive Control\n",
    "- Cross‑Entropy Method (CEM)\n",
    "(C) Policy verbessern\n",
    "Die Policy wird optimiert, indem sie im Modell ausprobiert wird:\n",
    "- Policy‑Gradient im Modell\n",
    "- Evolutionäre Suche\n",
    "- Trajectory Optimization\n",
    "\n",
    "### Typische Herausforderungen\n",
    "Model‑Based RL ist mächtig, aber nicht trivial:\n",
    "- Modellfehler akkumulieren sich bei langen Rollouts\n",
    "- Unsicherheit muss berücksichtigt werden\n",
    "- Planung kann rechenintensiv sein\n",
    "- Trade‑off: kurze, zuverlässige Rollouts vs. lange, informative Rollouts\n",
    "\n",
    "### Moderne, erfolgreiche Ansätze\n",
    "Diese Methoden zeigen, wie gut Model‑Based RL heute funktioniert:\n",
    "- Dreamer / DreamerV2 / DreamerV3\n",
    "Weltmodell + Latent‑Space‑Planung\n",
    "- MBPO (Model‑Based Policy Optimization)\n",
    "Kurze Rollouts → sehr stabil\n",
    "- PETS (Probabilistic Ensembles + Trajectory Sampling)\n",
    "Unsicherheitsmodellierung über Ensembles\n",
    "- PlaNet\n",
    "Latent‑Space‑Dynamikmodell\n",
    "\n",
    "### Kurzfassung\n",
    "Model‑Based RL ist wie ein lernender MPC: Der Agent baut sich ein Modell der Welt und plant damit seine Aktionen, bevor er sie ausführt.\n",
    "\n",
    "Der Agent lernt ein Modell der Umwelt und plant damit.\n",
    "Beispiele\n",
    "- Dyna‑Q\n",
    "- MBPO (Model‑Based Policy Optimization)\n",
    "- PETS (Probabilistic Ensembles with Trajectory Sampling)\n",
    "- Dreamer / DreamerV2 / DreamerV3\n",
    "- PlaNet\n",
    "Warum interessant?\n",
    "- Extrem sample‑effizient\n",
    "- Ermöglicht Planung, Imagination, Rollouts im Modell\n",
    "- Grundlage moderner Roboter‑RL‑Systeme\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f72aaa",
   "metadata": {},
   "source": [
    "# Hierachical RL (HRL)\n",
    "\n",
    "### Grundidee\n",
    "Statt eine einzige Policy zu lernen, lernt der Agent eine Hierarchie von Policies:\n",
    "- High‑Level Policy („Manager“)\n",
    "- entscheidet was getan werden soll\n",
    "- wählt Ziele, Subtasks oder „Options“\n",
    "- Low‑Level Policies („Worker“)\n",
    "- entscheiden wie das Ziel ausgeführt wird\n",
    "- steuern konkrete Aktionen\n",
    "Merksatz\n",
    "HRL zerlegt komplexe Aufgaben in überschaubare Teilaufgaben – wie ein Ingenieur, der ein System in Module gliedert.\n",
    "\n",
    "\n",
    "### Nutzen\n",
    "- Komplexe Aufgaben werden beherrschbar\n",
    "- Lange Zeithorizonte werden einfacher\n",
    "- Wiederverwendbare Skills entstehen\n",
    "- Effizientere Exploration durch strukturierte Subtasks\n",
    "- Bessere Generalisierung auf neue Aufgaben\n",
    "\n",
    "### Die zwei zentralen Konzepte\n",
    "(A) Options / Skills / Sub‑Policies\n",
    "Eine Option ist ein „Skill“, der aus drei Teilen besteht:\n",
    "- Initiation Set: Wann darf der Skill gestartet werden\n",
    "- Policy: Wie der Skill ausgeführt wird\n",
    "- Termination Condition: Wann der Skill endet\n",
    "Beispiele:\n",
    "- „Greife nach Objekt“\n",
    "- „Gehe zum Zielpunkt“\n",
    "- „Steige eine Treppe hoch“\n",
    "(B) High‑Level Controller\n",
    "Der High‑Level‑Agent wählt:\n",
    "- welchen Skill er ausführt\n",
    "- wann er ihn wechselt\n",
    "- welche Subziele verfolgt werden\n",
    "\n",
    "### Typische Herausforderungen\n",
    "HRL ist mächtig, aber bringt eigene Schwierigkeiten mit:\n",
    "- Subziele müssen sinnvoll definiert werden\n",
    "- Koordination zwischen Ebenen ist nicht trivial\n",
    "- Credit Assignment über lange Zeithorizonte bleibt anspruchsvoll\n",
    "- Training kann instabil sein, wenn Ebenen nicht harmonieren\n",
    "\n",
    "### Moderne, erfolgreiche Ansätze\n",
    "- Options Framework\n",
    "Klassische Theorie für Skills und Sub‑Policies\n",
    "- FeUdal Networks (FuN)\n",
    "High‑Level gibt Zielvektoren vor, Low‑Level folgt ihnen\n",
    "- HIRO (Hierarchical Reinforcement Learning with Off‑Policy Correction)\n",
    "Sehr erfolgreich in kontinuierlichen Steuerungsaufgaben\n",
    "- HAC (Hierarchical Actor‑Critic)\n",
    "Mehrere Ebenen von Actor‑Critic‑Policies\n",
    "\n",
    "### Kurzfassung\n",
    "\n",
    "Hierarchical RL ist wie ein mehrschichtiges Steuerungssystem: oben wird geplant, unten wird ausgeführt.\n",
    "\n",
    "Der Agent lernt auf mehreren Abstraktionsebenen.\n",
    "Beispiele\n",
    "- Options Framework\n",
    "- FeUdal Networks (FuN)\n",
    "- HIRO (Hierarchical Reinforcement Learning with Off‑policy Correction)\n",
    "Warum interessant?\n",
    "- Löst Langzeit‑Abhängigkeitsprobleme\n",
    "- Ermöglicht Skills, Sub‑Policies, Macro‑Actions\n",
    "- Sehr gut für Robotik und Navigation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f92fb8c",
   "metadata": {},
   "source": [
    "# Meta Reinforcement Learning\n",
    "\n",
    "### Grundidee\n",
    "Meta‑RL versucht nicht nur eine Policy für eine Aufgabe zu lernen, sondern eine Policy, die sich schnell an neue Aufgaben anpassen kann.\n",
    "Der Agent lernt also zwei Dinge:\n",
    "- Meta‑Wissen: Wie man effizient lernt\n",
    "- Task‑spezifisches Wissen: Wie man eine konkrete Aufgabe löst\n",
    "Merksatz\n",
    "Meta‑RL ist RL mit eingebauter Lernfähigkeit: Der Agent lernt, wie man lernt.\n",
    "\n",
    "\n",
    "### Nutzen\n",
    "- Schnelle Anpassung an neue Umgebungen\n",
    "- Wenige Samples nötig (Few‑Shot Learning)\n",
    "- Robustheit gegenüber Variationen in Dynamik oder Rewards\n",
    "- Generalisation über viele Aufgaben hinweg\n",
    "Das ist besonders relevant für Robotik, autonome Systeme und adaptive Steuerungen.\n",
    "\n",
    "### Die zwei zentralen Ansätze\n",
    "(A) Gradient‑basiertes Meta‑Learning (z. B. MAML‑RL)\n",
    "Die Idee:\n",
    "- Der Agent lernt Initialparameter, die sich mit wenigen Gradienten‑Schritten an eine neue Aufgabe anpassen lassen.\n",
    "- Meta‑Training: viele Aufgaben → gemeinsame Startparameter\n",
    "- Meta‑Test: neue Aufgabe → wenige Updates reichen\n",
    "Vorteil: Sehr schnelle Anpassung\n",
    "Nachteil: Rechenintensiv, empfindlich gegenüber Hyperparametern\n",
    "\n",
    "(B) Recurrent / Memory‑based Meta‑RL (z. B. RL²)\n",
    "Die Idee:\n",
    "- Der Agent bekommt eine RNN‑Policy, die über Episoden hinweg Informationen speichert.\n",
    "- Das RNN lernt, wie es aus Rewards und Beobachtungen die neue Aufgabe erkennt.\n",
    "Vorteil: Keine expliziten Gradienten‑Updates nötig\n",
    "Nachteil: Training kann instabil sein, da alles in der RNN‑Dynamik steckt\n",
    "\n",
    "### Typische Herausforderungen\n",
    "Meta‑RL ist mächtig, aber anspruchsvoll:\n",
    "- Viele Trainingsaufgaben nötig, um Meta‑Wissen zu lernen\n",
    "- Instabilität, wenn Aufgaben zu unterschiedlich sind\n",
    "- Exploration muss intelligent sein, um die Aufgabe schnell zu erkennen\n",
    "- Hohe Rechenlast bei gradient‑basierten Methoden\n",
    "\n",
    "### Moderne, erfolgreiche Ansätze\n",
    "- MAML‑RL (Model‑Agnostic Meta‑Learning)\n",
    "Meta‑Gradienten für schnelle Anpassung\n",
    "- RL² („RL‑Squared“) RNN‑Policy, die Lernen als Teil ihrer Dynamik implementiert\n",
    "- PEARL Probabilistischer Kontext für schnelle Task‑Inference\n",
    "- VariBAD Bayesianer Ansatz für Task‑Uncertainty\n",
    "\n",
    "### Kurzfassung\n",
    "\n",
    "Meta‑RL ist wie ein adaptiver Regler, der nicht nur die Regelung optimiert, sondern auch lernt, wie er sich bei neuen Aufgaben blitzschnell neu einstellt.\n",
    "\n",
    "Der Agent lernt, wie man lernt.\n",
    "Beispiele\n",
    "- MAML‑RL (Model‑Agnostic Meta‑Learning)\n",
    "- RL² (RL‑Squared)\n",
    "- PEARL\n",
    "Warum interessant?\n",
    "- Schnell anpassbare Policies\n",
    "- Wenige Samples für neue Aufgaben\n",
    "- Grundlage für „Generalist Agents“\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b297b19",
   "metadata": {},
   "source": [
    "# Offline/Batch RL\n",
    "\n",
    "### Grundidee\n",
    "Offline RL (auch Batch RL) bedeutet:\n",
    "Der Agent lernt ausschließlich aus aufgezeichneten Daten,\n",
    "ohne jemals mit der echten Umgebung zu interagieren.\n",
    "Die Daten stammen z. B. aus:\n",
    "- Logfiles eines Roboters\n",
    "- Fahrdaten eines autonomen Fahrzeugs\n",
    "- Produktionsdaten aus einer Anlage\n",
    "- Demonstrationen von Experten\n",
    "Merksatz\n",
    "Offline RL ist RL ohne Exploration – der Agent lernt nur aus vorhandenen Daten.\n",
    "\n",
    "\n",
    "### Nutzen\n",
    "- Sicherheit: Keine riskanten Aktionen im echten System\n",
    "- Kostenersparnis: Keine teuren Interaktionen (Robotik, Industrieanlagen)\n",
    "- Nutzung vorhandener Daten: Wie im klassischen Machine Learning\n",
    "- Industrie‑tauglich: Besonders relevant für Medizin, Fertigung, autonome Systeme\n",
    "Offline RL ist damit eine Brücke zwischen kontrollierter Datenverarbeitung und lernenden Steuerungen.\n",
    "\n",
    "### Die zentrale Herausforderung\n",
    "Der Agent darf keine Aktionen ausprobieren, die nicht im Datensatz vorkommen.\n",
    "Das führt zu zwei Problemen:\n",
    "(A) Distribution Shift\n",
    "Die Policy erzeugt Aktionen, die außerhalb der Datenverteilung liegen → das Q‑Modell extrapoliert falsch.\n",
    "(B) Overestimation\n",
    "Q‑Funktionen neigen dazu, unbekannte Aktionen zu überschätzen → führt zu instabilen Policies.\n",
    "Merksatz\n",
    "Offline RL scheitert nicht am Lernen, sondern an falschen Schätzungen für nie gesehene Aktionen.\n",
    "\n",
    "\n",
    "### Die drei Lösungsstrategien\n",
    "(A) Conservative Learning\n",
    "Die Q‑Funktion wird absichtlich „vorsichtig“ gemacht:\n",
    "- CQL (Conservative Q‑Learning)\n",
    "Bestraft Q‑Werte für nicht beobachtete Aktionen\n",
    "(B) Action Constraints\n",
    "Die Policy wird gezwungen, nahe an den Daten zu bleiben:\n",
    "- BCQ (Batch‑Constrained Q‑Learning)\n",
    "Policy darf nur Aktionen wählen, die ein generatives Modell als „wahrscheinlich“ einstuft\n",
    "- BEAR\n",
    "Minimiert die KL‑Distanz zur Daten‑Policy\n",
    "(C) Implicit Methods\n",
    "Die Policy wird indirekt aus den Daten gelernt:\n",
    "- IQL (Implicit Q‑Learning)\n",
    "Sehr stabil, da keine explizite Policy‑Constraint nötig ist\n",
    "\n",
    "### Moderne, erfolgreiche Ansätze\n",
    "- BCQ\n",
    "Verhindert Out‑of‑Distribution‑Aktionen\n",
    "- CQL\n",
    "Konservativer Q‑Lerner, sehr robust\n",
    "- IQL\n",
    "State‑of‑the‑art für viele Offline‑Benchmarks\n",
    "- AWAC\n",
    "Kombination aus Advantage‑Learning und Behavior Cloning\n",
    "- Decision Transformers\n",
    "Transformer‑basierte Sequenzmodelle für Offline RL\n",
    "\n",
    "### Kurzfassung\n",
    "Offline RL ist wie das Trainieren eines Reglers nur aus historischen Daten — ohne jemals das echte System zu berühren.\n",
    "\n",
    "Der Agent lernt ausschließlich aus aufgezeichneten Daten – ohne Interaktion.\n",
    "Beispiele\n",
    "- BCQ (Batch‑Constrained Q‑Learning)\n",
    "- CQL (Conservative Q‑Learning)\n",
    "- IQL (Implicit Q‑Learning)\n",
    "Warum interessant?\n",
    "- Relevanz für Industrie (Robotik, Medizin, autonome Fahrzeuge)\n",
    "- Kein Risiko durch Exploration\n",
    "- Nutzt große Datensätze wie supervised learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079c77cf",
   "metadata": {},
   "source": [
    "# Multi-Agent Reinforcement Learning\n",
    "\n",
    "### Grundidee\n",
    "Statt nur einen Agenten zu trainieren, lernen mehrere Agenten gleichzeitig, die:\n",
    "- kooperieren,\n",
    "- konkurrieren,\n",
    "- oder gemischte Ziele verfolgen.\n",
    "Jeder Agent hat:\n",
    "- eigene Beobachtungen\n",
    "- eigene Aktionen\n",
    "- eigene (oder gemeinsame) Rewards\n",
    "Merksatz\n",
    "MARL ist RL für Systeme mit mehreren intelligenten Akteuren, die miteinander interagieren.\n",
    "\n",
    "\n",
    "### Nutzen\n",
    "- Realistische Modellierung komplexer Systeme\n",
    "- Koordination zwischen Robotern, Fahrzeugen oder Maschinen\n",
    "- Skalierbarkeit auf große verteilte Systeme\n",
    "- Robustheit durch dezentrale Entscheidungen\n",
    "MARL ist damit ein Schlüsselkonzept für moderne autonome Systeme.\n",
    "\n",
    "### Die drei zentralen Herausforderungen\n",
    "(A) Non‑Stationarity\n",
    "Während ein Agent lernt, ändern sich die Policies der anderen → die Umgebung ist nicht mehr stationär.\n",
    "(B) Credit Assignment\n",
    "Wie verteilt man den Reward fair auf mehrere Agenten?\n",
    "(C) Skalierbarkeit\n",
    "Mehr Agenten → exponentiell mehr Interaktionen → Lernprozess wird komplex.\n",
    "Merksatz\n",
    "In MARL ist die Welt ständig in Bewegung, weil alle gleichzeitig lernen.\n",
    "\n",
    "\n",
    "### Die wichtigsten Lösungsansätze\n",
    "(A) Centralized Training, Decentralized Execution (CTDE)\n",
    "Während des Trainings:\n",
    "- alle Agenten teilen Informationen\n",
    "- ein zentrales Modell hilft beim Lernen\n",
    "Während der Ausführung:\n",
    "- jeder Agent handelt autonom\n",
    "- keine zentrale Instanz nötig\n",
    "Beispiele: QMIX, MADDPG, MAPPO\n",
    "\n",
    "(B) Value Decomposition\n",
    "Der gemeinsame Team‑Reward wird in individuelle Beiträge zerlegt.\n",
    "Beispiele:\n",
    "- VDN (Value Decomposition Networks)\n",
    "- QMIX (monotone Zerlegung)\n",
    "\n",
    "(C) Opponent Modeling\n",
    "Agenten lernen Modelle der anderen Agenten:\n",
    "- Vorhersage ihrer Aktionen\n",
    "- Strategische Anpassung\n",
    "Beispiele:\n",
    "- LOLA (Learning with Opponent‑Learning Awareness)\n",
    "\n",
    "(D) Kommunikation zwischen Agenten\n",
    "Agenten lernen, Informationen auszutauschen:\n",
    "- explizite Nachrichten\n",
    "- latente Kommunikationskanäle\n",
    "Beispiele:\n",
    "- CommNet\n",
    "- DIAL (Differentiable Inter-Agent Learning)\n",
    "\n",
    "### Moderne, erfolgreiche MARL‑Methoden\n",
    "- MADDPG\n",
    "Multi‑Agent‑Version von DDPG, sehr beliebt in kontinuierlichen Umgebungen\n",
    "- QMIX\n",
    "Wertzerlegung für kooperative Teams\n",
    "- MAPPO\n",
    "Multi‑Agent‑Variante von PPO, sehr stabil\n",
    "- VDN\n",
    "Einfache additive Zerlegung von Team‑Rewards\n",
    "- HATRPO / HAPPO\n",
    "Trust‑Region‑Methoden für Multi‑Agent‑Settings\n",
    "\n",
    "### Kurzfassung\n",
    "\n",
    "Multi‑Agent RL ist wie ein Team aus autonomen Reglern, die gleichzeitig lernen, miteinander zu kooperieren oder zu konkurrieren.\n",
    "\n",
    "Mehrere Agenten interagieren, kooperieren oder konkurrieren.\n",
    "Beispiele\n",
    "- MADDPG (Multi‑Agent DDPG)\n",
    "- QMIX\n",
    "- VDN (Value Decomposition Networks)\n",
    "- MAPPO (Multi‑Agent PPO)\n",
    "Warum interessant?\n",
    "- Modelliert reale Systeme: Verkehr, Roboterschwärme, Spiele\n",
    "- Komplexe Dynamiken: Kooperation, Konkurrenz, Kommunikation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dc612d",
   "metadata": {},
   "source": [
    "# Imitation Learning und Inverse Learning\n",
    "\n",
    "Imitation Learning & Inverse Reinforcement Learning – Die Essenz in klarer Struktur\n",
    "### Grundidee\n",
    "Beide Methoden nutzen Demonstrationen eines Experten, z. B.:\n",
    "- Fahrdaten eines menschlichen Fahrers\n",
    "- Bewegungen eines Roboters\n",
    "- Bedienabläufe einer Maschine\n",
    "Der Unterschied:\n",
    "- Imitation Learning (IL):\n",
    "Der Agent lernt direkt das Verhalten des Experten nachzuahmen.\n",
    "- Inverse Reinforcement Learning (IRL):\n",
    "Der Agent versucht zuerst herauszufinden, welche Belohnungsfunktion der Experte optimiert hat — und lernt dann selbst.\n",
    "Merksatz\n",
    "IL imitiert das Verhalten. IRL rekonstruiert die Motivation.\n",
    "\n",
    "\n",
    "### Nutzen\n",
    "- Keine gefährliche Exploration nötig\n",
    "- Schnelles Lernen aus vorhandenen Daten\n",
    "- Einfacher Einstieg in RL für reale Systeme\n",
    "- Übertragbarkeit von Expertenwissen auf autonome Systeme\n",
    "Besonders relevant für Robotik, autonome Fahrzeuge, Fertigung, Medizin.\n",
    "\n",
    "### Imitation Learning (IL)\n",
    "#### Grundidee\n",
    "Der Agent lernt eine Policy, die möglichst gut die Expertenaktionen reproduziert.\n",
    "Zwei Hauptformen:\n",
    "(A) Behavior Cloning (BC)\n",
    "- Supervised Learning:\n",
    "$$\\pi (a|s)\\approx \\pi _{\\mathrm{Expert}}(a|s)$$\n",
    "- Einfach, schnell, aber anfällig für Fehlerakkumulation.\n",
    "(B) DAgger (Dataset Aggregation)\n",
    "- Der Agent sammelt eigene Daten\n",
    "- Der Experte korrigiert Fehler\n",
    "- Sehr robust, deutlich besser als BC\n",
    "Vorteile\n",
    "- Sehr einfach\n",
    "- Keine RL‑Instabilitäten\n",
    "- Funktioniert gut bei klaren Demonstrationen\n",
    "Nachteile\n",
    "- Kein Verständnis der Zielsetzung\n",
    "- Fehler verstärken sich über Zeit (bei BC)\n",
    "\n",
    "### Inverse Reinforcement Learning (IRL)\n",
    "#### Grundidee\n",
    "Der Agent versucht herauszufinden:\n",
    "Welche Belohnungsfunktion muss der Experte optimiert haben, damit sein Verhalten optimal erscheint?\n",
    "\n",
    "Erst danach wird eine Policy gelernt.\n",
    "Warum das sinnvoll ist?\n",
    "- Experten demonstrieren oft Ziele, nicht nur Aktionen\n",
    "- IRL extrahiert Motivation, nicht nur Verhalten\n",
    "- Dadurch kann der Agent besser generalisieren\n",
    "Wichtige Methoden:\n",
    "(A) MaxEnt IRL (Maximum Entropy IRL)\n",
    "- Beliebteste klassische Methode\n",
    "- Sucht die Reward‑Funktion, die das Expertenverhalten am wahrscheinlichsten macht\n",
    "(B) AIRL (Adversarial IRL)\n",
    "- Nutzt GAN‑ähnliche Architektur\n",
    "- Sehr leistungsfähig, gut für komplexe Umgebungen\n",
    "(C) GAIL (Generative Adversarial Imitation Learning)\n",
    "- Verbindet IL und IRL\n",
    "- Lernt direkt eine Policy, ohne explizite Reward‑Funktion\n",
    "- Sehr erfolgreich in Robotik‑Benchmarks\n",
    "\n",
    "### Typische Herausforderungen\n",
    "Imitation Learning\n",
    "- Fehlerakkumulation\n",
    "- Schlechte Generalisierung außerhalb der Demonstrationen\n",
    "Inverse RL\n",
    "- Reward‑Identifikation ist oft nicht eindeutig\n",
    "- Rechenintensiv\n",
    "- Benötigt viele Demonstrationen\n",
    "\n",
    "### Moderne, erfolgreiche Ansätze\n",
    "- GAIL\n",
    "GAN‑basiertes Imitation Learning, sehr stabil\n",
    "- AIRL\n",
    "Liefert explizite Reward‑Funktionen\n",
    "- BC + RL Fine‑Tuning\n",
    "Erst imitieren, dann optimieren\n",
    "- Diffusion‑based Imitation Learning\n",
    "Moderne generative Modelle für komplexe Bewegungen\n",
    "\n",
    "### Kurzfassung\n",
    "\n",
    "Imitation Learning kopiert das Verhalten eines Experten, während Inverse RL versucht zu verstehen, warum der Experte so handelt — und daraus eine eigene optimale Policy ableitet.\n",
    "\n",
    "Der Agent lernt aus Demonstrationen.\n",
    "Beispiele\n",
    "- GAIL (Generative Adversarial Imitation Learning)\n",
    "- AIRL (Adversarial Inverse RL)\n",
    "- DAgger\n",
    "Warum interessant?\n",
    "- Sehr praxisrelevant (Robotik, autonome Systeme)\n",
    "- Weniger Trial‑and‑Error\n",
    "- Verbindung zu Supervised Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae0c3e8",
   "metadata": {},
   "source": [
    "# Generalists / Foundation RL\n",
    "\n",
    "### Grundidee\n",
    "Generalist RL verfolgt das Ziel, einen einzigen Agenten zu trainieren, der:\n",
    "- viele verschiedene Aufgaben lösen kann\n",
    "- in verschiedenen Umgebungen\n",
    "- mit verschiedenen Modalitäten (Bilder, Text, Aktionen, Sprache, Roboterbefehle)\n",
    "- ohne für jede Aufgabe neu trainiert zu werden\n",
    "Der Agent wird also nicht für eine Aufgabe optimiert, sondern für eine ganze Klasse von Aufgaben.\n",
    "Merksatz\n",
    "Generalist RL ist ein universeller Agent, der viele Aufgaben beherrscht — ähnlich wie ein Mensch.\n",
    "\n",
    "\n",
    "### Nutzen\n",
    "- Ein Modell für viele Aufgaben statt viele spezialisierte Modelle\n",
    "- Transferfähigkeit zwischen Aufgaben\n",
    "- Robustheit gegenüber Variationen\n",
    "- Skalierbarkeit durch große Datensätze\n",
    "- Realwelt‑Tauglichkeit für Robotik, autonome Systeme, Fertigung\n",
    "Generalist RL ist ein Schritt in Richtung General Intelligence im technischen Sinne.\n",
    "\n",
    "### Die drei zentralen Prinzipien\n",
    "(A) Multi‑Task Learning\n",
    "Der Agent wird gleichzeitig auf vielen Aufgaben trainiert:\n",
    "- Navigation\n",
    "- Greifen\n",
    "- Manipulation\n",
    "- Spiele\n",
    "- Sprachbefehle\n",
    "Dadurch lernt er generelle Strategien, nicht nur Speziallösungen.\n",
    "\n",
    "(B) Multi‑Modalität\n",
    "Generalist Agents verarbeiten verschiedene Eingaben:\n",
    "- Bilder\n",
    "- Text\n",
    "- Sensordaten\n",
    "- Aktionssequenzen\n",
    "- Sprache\n",
    "Und erzeugen verschiedene Ausgaben:\n",
    "- Aktionen\n",
    "- Text\n",
    "- Steuerbefehle\n",
    "\n",
    "(C) Sequenzmodellierung\n",
    "Viele Generalist‑Ansätze nutzen Transformer‑Modelle, die RL als Sequenzproblem formulieren:\n",
    "- Zustand → Aktion → Reward → nächster Zustand\n",
    "- Alles wird als Token‑Sequenz behandelt\n",
    "Das ermöglicht:\n",
    "- große Datensätze\n",
    "- einheitliche Architektur\n",
    "- starke Generalisierung\n",
    "\n",
    "### Typische Herausforderungen\n",
    "(A) Datenvielfalt\n",
    "Generalist Agents benötigen riesige, diverse Datensätze.\n",
    "(B) Skalierung\n",
    "Training ist rechenintensiv und erfordert große Modelle.\n",
    "(C) Konsistenz\n",
    "Der Agent muss lernen, wann welche Fähigkeit relevant ist.\n",
    "(D) Sicherheit\n",
    "Generalist Agents müssen zuverlässig und vorhersehbar handeln.\n",
    "\n",
    "### Moderne, erfolgreiche Ansätze\n",
    "(A) Gato (DeepMind)\n",
    "- Ein Modell für 600+ Aufgaben\n",
    "- Steuerung von Robotern, Spielen, Textverarbeitung\n",
    "- Transformer‑basiert\n",
    "(B) RT‑1 / RT‑2 (Google Robotics Transformer)\n",
    "- Roboter, die aus multimodalen Daten lernen\n",
    "- RT‑2 verbindet Vision, Sprache und Aktionen\n",
    "- Sehr leistungsfähig in realen Robotik‑Tasks\n",
    "(C) Decision Transformer\n",
    "- RL als Sequenzmodellierung\n",
    "- Kein Value‑Learning nötig\n",
    "- Funktioniert gut in Offline‑RL‑Settings\n",
    "(D) Generalist Agents für Spiele\n",
    "- Multi‑Game‑Agents\n",
    "- Multi‑Modal‑Policies\n",
    "- Zero‑shot‑Transfer zwischen Spielen\n",
    "\n",
    "### Kurzfassung\n",
    "\n",
    "Generalist RL ist wie ein universeller Steuerungsagent, der aus vielen Aufgaben lernt und sein Wissen flexibel auf neue Situationen überträgt.\n",
    "\n",
    "Große Modelle, die viele Aufgaben gleichzeitig lösen.\n",
    "Beispiele\n",
    "- Gato (DeepMind)\n",
    "- RT‑1 / RT‑2 (Google Robotics Transformer)\n",
    "- OpenAI Gym‑Generalist Agents\n",
    "Warum interessant?\n",
    "- Verbindung von RL, Transformers und großen Datensätzen\n",
    "- Richtung „General Intelligence“\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5a969e",
   "metadata": {},
   "source": [
    "# Übersicht\n",
    "| Kategorie | Beispiele | Kernidee | Nutzen |\n",
    "|----------|-----------|----------|--------|\n",
    "| Model‑Based RL | Dreamer, MBPO, PETS | Weltmodell + Planung | Sample‑Effizienz, Robotik |\n",
    "| Distributional RL | C51, QR‑DQN, IQN | Return‑Verteilung statt Erwartungswert | Stabilität, Exploration |\n",
    "| Hierarchical RL | Options, HIRO, FuN | Skills, Sub‑Policies | Langzeitplanung |\n",
    "| Meta‑RL | MAML‑RL, RL², PEARL | Lernen zu lernen | Schnell adaptierbare Policies |\n",
    "| Offline RL | BCQ, CQL, IQL | Lernen aus Daten ohne Interaktion | Industrie‑relevant |\n",
    "| Multi‑Agent RL | MADDPG, QMIX, MAPPO | Mehrere Agenten | Kooperation/Konkurrenz |\n",
    "| Exploration | RND, ICM, Go‑Explore | Intrinsische Motivation | Sparse Rewards |\n",
    "| Imitation / IRL | GAIL, AIRL, DAgger | Lernen aus Demonstrationen | Robotik, autonome Systeme |\n",
    "| Generalist RL | Gato, RT‑2 | Multi‑Task‑Agents | Richtung AGI |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa682ef2",
   "metadata": {},
   "source": [
    "# Evolutionary Learning"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
