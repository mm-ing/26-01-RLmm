# Q-Learning
1. Grundidee: Qâ€‘Learning mit Funktionsapproximation
Qâ€‘Learning aktualisiert Werte nach:
$$Q(s,a)\leftarrow r+\gamma \max _{a'}Q(s',a')$$
Bei groÃŸen oder kontinuierlichen ZustandsrÃ¤umen ist eine Tabelle unmÃ¶glich â†’ ein Neural Network approximiert Q.

2. Das Qâ€‘Netzwerk
Das Netzwerk bekommt den Zustand s als Input und gibt fÃ¼r jede Aktion einen Qâ€‘Wert aus:
- Input: Zustand (z.â€¯B. Position, Geschwindigkeit)
- Output: Vektor der Qâ€‘Werte fÃ¼r alle Aktionen
Beispiel CartPole:
Input: 4 Werte â†’ Output: 2 Qâ€‘Werte (left, right)

3. Experience Replay (Replay Buffer)
Statt jede Transition sofort zu lernen, speichert DQN sie in einem Replay Buffer:
$$(s,a,r,s',\mathrm{done})$$
Beim Training werden zufÃ¤llige Batches gezogen.
Vorteile:
- Bricht Korrelationen zwischen Samples
- Stabilisiert das Lernen
- ErhÃ¶ht Sampleâ€‘Effizienz
Das ist einer der entscheidenden Tricks, warum DQN funktioniert.

4. Target Network
Ein zweites Netzwerk, das langsam aktualisiert wird, liefert stabile Zielwerte:
$$y=r+\gamma \max _{a'}Q_{\mathrm{target}}(s',a')$$
Warum?
- Verhindert, dass das Netz seine eigenen Ziele stÃ¤ndig verschiebt
- Reduziert Divergenz
Das Targetâ€‘Netz wird alle N Schritte kopiert oder â€soft updatedâ€œ.

5. Lossâ€‘Funktion
Das Training minimiert den TDâ€‘Error:
$$L=\left( Q_{\mathrm{online}}(s,a)-y\right) ^2$$
Das ist ein klassischer MSEâ€‘Loss.

6. Exploration: Îµâ€‘Greedy
WÃ¤hrend des Trainings:
- Mit Wahrscheinlichkeit Îµ â†’ zufÃ¤llige Aktion
- Mit Wahrscheinlichkeit 1âˆ’Îµ â†’ beste Aktion laut Qâ€‘Netz
Îµ wird typischerweise Ã¼ber Zeit reduziert (Îµâ€‘Decay).

7. Trainingsschleife (kompakt)
Eine Episode lÃ¤uft so:
- Zustand s beobachten
- Aktion a wÃ¤hlen (Îµâ€‘greedy)
- Schritt ausfÃ¼hren â†’ r,s'
- Transition in Replay Buffer speichern
- Miniâ€‘Batch aus Replay Buffer ziehen
- Zielwert y mit Targetâ€‘Netz berechnen
- Onlineâ€‘Netz trainieren
- Targetâ€‘Netz periodisch aktualisieren
Das wiederholt sich Ã¼ber viele Episoden.

ğŸ§­ Warum funktioniert DQN so gut?
Weil es drei Probleme lÃ¶st, die klassisches Qâ€‘Learning mit NN instabil machen:

![alt text](q_learning.png)


Diese drei Mechanismen machen DQN zu einem robusten RLâ€‘Algorithmus.

ğŸ® Beispiel: Was macht das Netz eigentlich?
Wenn das Netz lernt, dass â€Pole fÃ¤llt nach rechtsâ€œ, dann steigt der Qâ€‘Wert fÃ¼r Aktion â€leftâ€œ.
Das Netz lernt also WertschÃ¤tzungen, nicht direkt Aktionen.

ğŸ§  Kurzfassung in einem Satz
Ein DQN approximiert die Qâ€‘Funktion mit einem neuronalen Netz, stabilisiert das Lernen durch Replay Buffer und Targetâ€‘Netzwerke und wÃ¤hlt Aktionen Ã¼ber Îµâ€‘Greedy.


# Verbesserungen

1. Verbesserungen am Qâ€‘Learningâ€‘Update
ğŸ”· Double DQN (DDQN)
- Trennt Aktionswahl und Aktionsbewertung
- Reduziert Overestimation Bias
- Sehr einfache Erweiterung, groÃŸer Effekt

ğŸ”· Dueling DQN
- Zerlegt Q(s,a) in:
- V(s) = Wert des Zustands
- A(s,a) = Vorteil der Aktion
- Hilft besonders in Umgebungen, in denen viele Aktionen Ã¤hnlich gut sind
- Stabilisiert das Lernen

ğŸ”· Averaged-DQN
- Nutzt mehrere Qâ€‘Netze und mittelt deren SchÃ¤tzungen
- Reduziert Varianz und Overestimation
- Teurer, aber robuster

ğŸ”· Maxmin DQN
- Mehrere Qâ€‘Netze, aber nimmt das Minimum
- Noch stÃ¤rkere Kontrolle von Overestimation

ğŸ§© 2. Verbesserungen am Replay Buffer
ğŸ”¶ Prioritized Experience Replay (PER)
- WÃ¤hlt wichtige Transitionen hÃ¤ufiger
- Beschleunigt Lernen deutlich
- Nutzt TDâ€‘Error als PrioritÃ¤t

ğŸ”¶ Nâ€‘Step Replay
- Nutzt nâ€‘Schrittâ€‘Returns statt 1â€‘Step
- Bessere Credit Assignment
- HÃ¤ufig kombiniert mit PER

ğŸ”¶ Replay Buffer mit Segmentierung / Reservoir Sampling
- FÃ¼r Nonâ€‘stationÃ¤re Umgebungen
- Verhindert, dass alte Daten zu dominant werden

ğŸ§© 3. Verbesserungen an der Exploration
ğŸ”µ Noisy DQN
- Ersetzt Îµâ€‘Greedy durch stochastische, lernbare Rauschschichten
- Exploration wird Teil des Modells
- Sehr elegant und oft besser als Îµâ€‘Greedy

ğŸ”µ Bootstrapped DQN
- Mehrere Qâ€‘Heads, zufÃ¤llige Maskierung
- Liefert Thompson Samplingâ€‘Ã¤hnliche Exploration
- Sehr stark in komplexen Umgebungen

ğŸ§© 4. Verbesserungen an der ZielwertschÃ¤tzung
ğŸŸ£ Distributional DQN (C51)
- Lernt nicht nur den Erwartungswert Q(s,a), sondern die ganze Verteilung
- FÃ¼hrt zu stabilerem und oft besserem Verhalten
- Grundlage fÃ¼r viele moderne Agenten

ğŸŸ£ Quantile Regression DQN (QRâ€‘DQN)
- Approximiert die Verteilung Ã¼ber Quantile
- Flexibler als C51
- Grundlage fÃ¼r IQN

ğŸŸ£ Implicit Quantile Networks (IQN)
- Kontinuierliche Quantile
- Sehr leistungsfÃ¤hig
- Stateâ€‘ofâ€‘theâ€‘art in vielen Atariâ€‘Benchmarks

ğŸ§© 5. Kombinationen der besten Ideen
ğŸŸ¢ Rainbow DQN
Die â€Allâ€‘Starâ€‘Versionâ€œ von DQN.
Kombiniert:
- Double DQN
- Dueling
- Prioritized Replay
- Multiâ€‘Step Learning
- Distributional RL (C51)
- Noisy Nets
- (teilweise) L2â€‘Regularisierung
Rainbow ist heute der Deâ€‘factoâ€‘Standard fÃ¼r DQNâ€‘basierte Agenten.

ğŸ§© 6. Erweiterungen fÃ¼r spezielle Szenarien
ğŸŸ  Recurrent DQN (DRQN)
- Nutzt LSTM/GRU
- FÃ¼r POMDPs (teilweise beobachtbare Umgebungen)
- Z.â€¯B. bei visuellen RLâ€‘Tasks mit Flickering Frames

ğŸŸ  Parametric DQN
- FÃ¼r Umgebungen mit variablen AktionsrÃ¤umen
- Z.â€¯B. Empfehlungssysteme, Dialogsysteme

ğŸŸ  Multiâ€‘Agent DQNâ€‘Varianten
- VDN, QMIX (nicht direkt DQN, aber Qâ€‘Learningâ€‘basiert)
- FÃ¼r kooperative Multiâ€‘Agentâ€‘Settings

ğŸ§­ Wenn du das Ganze in einer Lernpipeline nutzen willst
Eine sinnvolle Progression wÃ¤re:
- DQN
- Double DQN
- Dueling + PER
- Noisy Nets
- Distributional (C51 oder QRâ€‘DQN)
- Rainbow
- Bootstrapped / IQN, je nach Forschungsinteresse
