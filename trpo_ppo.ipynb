{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7371ddc",
   "metadata": {},
   "source": [
    "# PPO\n",
    "PPO (Proximal Policy Optimization) ist im Kern eine vereinfachte, robuste Weiterentwicklung von TRPO â€“ mit fast derselben Idee, aber ohne komplizierte Optimierung. Genau deshalb ist PPO heute der Deâ€‘factoâ€‘Standard im Policyâ€‘Gradientâ€‘RL.\n",
    "\n",
    "## Grundidee von PPO\n",
    "PPO will Policyâ€‘Updates stabil halten, indem es verhindert, dass die neue Policy zu weit von der alten abweicht â€“ aber ohne die schweren mathematischen Constraints von TRPO.\n",
    "Der Trick:\n",
    "PPO begrenzt das Policyâ€‘Update direkt im Lossâ€‘Term durch Clipping.\n",
    "\n",
    "Damit wird jeder Update-Schritt automatisch â€žvertrÃ¤glichâ€œ.\n",
    "\n",
    "## Der Kern: Clipped Surrogate Objective\n",
    "PPO maximiert eine modifizierte Version des Policyâ€‘Gradientâ€‘Ziels:\n",
    "$$L(\\theta )=\\mathbb{E}\\left[ \\min \\left( r(\\theta )A,\\; \\mathrm{clip}(r(\\theta ),1-\\epsilon ,1+\\epsilon )A\\right) \\right] $$\n",
    "mit\n",
    "$$r(\\theta )=\\frac{\\pi _{\\theta }(a|s)}{\\pi _{\\theta _{\\mathrm{old}}}(a|s)}$$\n",
    "Was bedeutet das?\n",
    "r$$(\\theta )$$ \n",
    "misst, wie stark sich die neue Policy von der alten unterscheidet.\n",
    "- Wenn \n",
    "$$r(\\theta )$$\n",
    " zu groÃŸ oder zu klein wird, wird es geclippt.\n",
    "- Dadurch kann der Update nicht explodieren.\n",
    "Ergebnis:\n",
    "Die Policy verbessert sich, aber nur in einem â€žsicherenâ€œ Bereich.\n",
    "\n",
    "## Warum ist das besser als TRPO?\n",
    "TRPO braucht:\n",
    "- KLâ€‘Constraint\n",
    "- Conjugate Gradient\n",
    "- Line Search\n",
    "PPO ersetzt das alles durch einfaches Clipping im Loss.\n",
    "Das ist:\n",
    "- schneller\n",
    "- stabiler\n",
    "- GPUâ€‘freundlich\n",
    "- leichter zu implementieren\n",
    "Und liefert trotzdem fast dieselbe StabilitÃ¤t wie TRPO.\n",
    "\n",
    "ðŸ”§ PPOâ€‘Ablauf (vereinfacht)\n",
    "- Rollouts sammeln\n",
    "â€“ mehrere Schritte in der Umgebung ausfÃ¼hren\n",
    "â€“ ZustÃ¤nde, Aktionen, Rewards speichern\n",
    "- Advantage schÃ¤tzen\n",
    "â€“ oft mit GAE (Generalized Advantage Estimation)\n",
    "- Policyâ€‘Update mit Clipped Loss\n",
    "â€“ mehrere Epochs Ã¼ber dieselben Daten\n",
    "â€“ Miniâ€‘Batchâ€‘SGD\n",
    "- Valueâ€‘Function separat optimieren\n",
    "â€“ eigener Loss (MSE)\n",
    "- Optional: KLâ€‘Monitoring\n",
    "â€“ PPO bricht ab, wenn KL zu groÃŸ wird\n",
    "\n",
    "## Warum PPO so beliebt ist\n",
    "- extrem stabil\n",
    "- sehr sampleâ€‘effizient\n",
    "- funktioniert in kontinuierlichen AktionsrÃ¤umen\n",
    "- einfach zu implementieren\n",
    "- robust gegenÃ¼ber Hyperparametern\n",
    "- funktioniert in fast allen RLâ€‘Benchmarks\n",
    "Deshalb nutzen es viele Frameworks (SB3, RLlib, CleanRL) als Standard.\n",
    "\n",
    "## Kurzfassung\n",
    "PPO funktioniert so:\n",
    "- Nutzt denselben Vorteil wie TRPO: begrenzte Policyâ€‘Ã„nderung\n",
    "- Aber statt komplizierter Constraints nutzt es Clipping im Loss\n",
    "- Dadurch wird jeder Update automatisch stabil\n",
    "- Sehr effizient und leicht zu trainieren"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9f5bc2",
   "metadata": {},
   "source": [
    "# TRPO\n",
    "TRPO (â€žTrust Region Policy Optimizationâ€œ) ist eine der elegantesten Policyâ€‘Gradientâ€‘Methoden, weil sie ein zentrales Problem klassischer PGâ€‘Verfahren lÃ¶st: zu groÃŸe, instabile Policyâ€‘Updates. Die Idee ist, die Policy nur so weit zu verÃ¤ndern, wie man ihr â€žvertrauenâ€œ kann â€“ daher Trust Region.\n",
    "\n",
    "TRPO maximiert den erwarteten Vorteil (Advantage), unter der Nebenbedingung, dass sich die neue Policy nicht zu stark von der alten unterscheidet.\n",
    "Formal:\n",
    "$$\\max _{\\theta }\\; \\mathbb{E_{\\mathnormal{\\pi _{\\theta _{\\mathrm{old}}}}}}\\left[ \\frac{\\pi _{\\theta }(a|s)}{\\pi _{\\theta _{\\mathrm{old}}}(a|s)}A(s,a)\\right] $$\n",
    "unter der Nebenbedingung:\n",
    "$$D_{\\mathrm{KL}}(\\pi _{\\theta _{\\mathrm{old}}}\\, \\| \\, \\pi _{\\theta })\\leq \\delta $$\n",
    "Das heiÃŸt:\n",
    "- Ziel: verbessere die Policy proportional zum Advantage\n",
    "- Constraint: KLâ€‘Divergenz darf nur klein sein (kleiner als Î´)\n",
    "Damit verhindert TRPO â€žPolicyâ€‘Explosionenâ€œ.\n",
    "\n",
    "## Warum braucht man so etwas?\n",
    "Normale Policyâ€‘Gradients machen:\n",
    "- einen Schritt in Richtung des Gradienten\n",
    "- mit einer festen Lernrate\n",
    "Das Problem:\n",
    "Ein zu groÃŸer Schritt kann die Policy komplett ruinieren â†’ Training kollabiert.\n",
    "TRPO lÃ¶st das, indem es mathematisch garantiert, dass jeder Schritt die Policy nur kontrolliert verÃ¤ndert.\n",
    "\n",
    "## Wie TRPO das technisch lÃ¶st\n",
    "TRPO nutzt drei zentrale Bausteine:\n",
    "\n",
    "1. Surrogate Objective\n",
    "Statt den echten Return zu maximieren, verwendet TRPO eine â€žErsatzfunktionâ€œ:\n",
    "$$L(\\theta )=\\mathbb{E}\\left[ r(\\theta )A\\right] $$\n",
    "mit dem WahrscheinlichkeitsverhÃ¤ltnis:\n",
    "$$r(\\theta )=\\frac{\\pi _{\\theta }(a|s)}{\\pi _{\\theta _{\\mathrm{old}}}(a|s)}$$\n",
    "Das ist derselbe Kern wie spÃ¤ter bei PPO.\n",
    "\n",
    "2. Trustâ€‘Regionâ€‘Constraint (KLâ€‘Bound)\n",
    "Die Policy darf sich nur wenig Ã¤ndern:\n",
    "$$D_{\\mathrm{KL}}(\\pi _{\\mathrm{old}}\\, \\| \\, \\pi _{\\theta })\\leq \\delta $$\n",
    "Das ist ein harte Nebenbedingung (Constraint Optimization).\n",
    "\n",
    "3. Conjugate Gradient + Line Search\n",
    "Weil neuronale Netze viele Parameter haben, kann man die Optimierung nicht direkt lÃ¶sen.\n",
    "TRPO nutzt:\n",
    "- Conjugate Gradient, um die natÃ¼rliche Gradientenrichtung zu approximieren\n",
    "- Backtracking Line Search, um sicherzustellen, dass der KLâ€‘Constraint eingehalten wird\n",
    "Das Ergebnis ist ein sicherer, monotonic improving Policyâ€‘Update.\n",
    "\n",
    "## Warum ist TRPO wichtig?\n",
    "TRPO war der erste groÃŸe Durchbruch, der gezeigt hat:\n",
    "Man kann Policyâ€‘Gradients stabilisieren, indem man die Policyâ€‘Ã„nderung begrenzt.\n",
    "\n",
    "Das fÃ¼hrte direkt zu:\n",
    "- PPO (vereinfachte Version, heute Standard)\n",
    "- Soft Actorâ€‘Critic (Ã¤hnliche StabilitÃ¤tsideen)\n",
    "- Natural Policy Gradientâ€‘Varianten\n",
    "\n",
    "## Kurzfassung\n",
    "TRPO funktioniert so:\n",
    "- Berechne Advantageâ€‘SchÃ¤tzungen\n",
    "- Formuliere ein Surrogate Objective\n",
    "- Finde einen Policyâ€‘Update, der dieses Objective verbessert\n",
    "- Aber: Erlaube nur kleine KLâ€‘Ã„nderungen\n",
    "- Nutze Conjugate Gradient + Line Search, um den Constraint einzuhalten\n",
    "Ergebnis: stabile, monotone Policyâ€‘Verbesserung.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
