{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b7de511",
   "metadata": {},
   "source": [
    "asdf\n",
    "\n",
    "# Policy Gradient Methoden\n",
    "\n",
    "Struktur:\n",
    "\n",
    "- Was optimiert die Methode?\n",
    "- Wie stabilisiert sie das Lernen?\n",
    "- Welche Zusatztricks nutzt sie?\n",
    "\n",
    "### 1. VPG – Vanilla Policy Gradient\n",
    "„Der einfachste, direkteste Policy‑Gradient.“\n",
    "\n",
    "Kernidee\n",
    "- Wir schätzen den Gradienten der erwarteten Return‑Funktion und gehen direkt in diese Richtung.\n",
    "- Keine Normalisierung, keine Constraints, keine Tricks.\n",
    "\n",
    "Wesentliche Eigenschaften\n",
    "- Einfach, aber instabil: hohe Varianz, empfindlich gegenüber Lernrate.\n",
    "- Policy wird direkt aktualisiert: keine Rücksicht auf die Geometrie des Parameterraums.\n",
    "- Baseline/Advantage kann Varianz reduzieren, ist aber optional.\n",
    "\n",
    "Merksatz\n",
    "- VPG ist der rohe Policy‑Gradient: funktioniert, aber schwankt stark.\n",
    "\n",
    "\n",
    "### 2. NPG – Natural Policy Gradient\n",
    "„Wie VPG, aber mit besserem Schritt – wir gehen nicht blind, sondern entlang der wahren Geometrie.“\n",
    "\n",
    "Kernidee\n",
    "- Statt eines normalen Gradienten nutzt NPG den Natural Gradient, der die Krümmung des Policy‑Raums berücksichtigt.\n",
    "- Das entspricht einem Fisher‑Information‑gewichteten Schritt.\n",
    "\n",
    "Wesentliche Eigenschaften\n",
    "- Stabilere Updates als VPG.\n",
    "- Richtungswahl ist ‚natürlicher‘: gleiche KL‑Änderung bedeutet gleiche „Distanz“ im Policy‑Raum.\n",
    "\n",
    "- Teurer als VPG, weil Fisher‑Matrix geschätzt werden muss.\n",
    "Merksatz\n",
    "- NPG ist VPG mit einem intelligenteren Schritt – weniger Varianz, bessere Richtung.\n",
    "\n",
    "\n",
    "### 3. A2C – Advantage Actor‑Critic\n",
    "„Wir kombinieren Policy‑Gradient mit einem Wertschätzer, um die Varianz zu reduzieren.“\n",
    "\n",
    "Kernidee\n",
    "- Zwei Netze:\n",
    "- Actor: die Policy\n",
    "- Critic: schätzt den Value oder Advantage\n",
    "- Der Critic liefert eine bessere Schätzung des Gradienten → weniger Varianz.\n",
    "\n",
    "Wesentliche Eigenschaften\n",
    "- Synchronisierte Updates (im Gegensatz zu A3C).\n",
    "- Stabiler als VPG, aber immer noch on‑policy.\n",
    "- Advantage sorgt für zielgerichtete Updates.\n",
    "\n",
    "Merksatz\n",
    "- A2C ist VPG mit eingebautem Berater (Critic), der sagt, wie gut eine Aktion wirklich war.\n",
    "\n",
    "\n",
    "### 4. SAC – Soft Actor‑Critic\n",
    "„Moderne Off‑Policy‑Methode mit Entropiebonus – lernt stabil, explorativ und effizient.“\n",
    "\n",
    "Kernidee\n",
    "- Maximiert nicht nur den Return, sondern auch die Entropie der Policy.\n",
    "- Off‑policy mit Replay Buffer → hohe Sample‑Effizienz.\n",
    "- Zwei Q‑Funktionen zur Stabilisierung (Double‑Q‑Trick).\n",
    "\n",
    "Wesentliche Eigenschaften\n",
    "- Sehr stabil, auch in kontinuierlichen Räumen.\n",
    "- Exploration eingebaut durch Entropieterm.\n",
    "- Off‑policy → Daten werden wiederverwendet.\n",
    "- State of the art für viele kontinuierliche Steuerungsaufgaben.\n",
    "\n",
    "Merksatz\n",
    "- SAC ist der moderne Allrounder: stabil, explorativ, sample‑effizient.\n",
    "\n",
    "\n",
    "### 5. TRPO – Trust Region Policy Optimization\n",
    "„Wir machen große Fortschritte, aber nur wenn wir sicher sind, dass die Policy nicht zu weit springt.“\n",
    "\n",
    "Kernidee\n",
    "- TRPO maximiert den Return unter einer harten Nebenbedingung:\n",
    "Die neue Policy darf sich nur begrenzt von der alten unterscheiden (KL‑Constraint).\n",
    "- Dadurch werden zu große, destruktive Updates verhindert.\n",
    "\n",
    "Wesentliche Eigenschaften\n",
    "- Sehr stabil, da Updates garantiert „sicher“ sind.\n",
    "- Nutzt den Natural Gradient und löst ein constrained optimization problem.\n",
    "- Teuer: benötigt Conjugate Gradient und line search.\n",
    "- Liefert oft monotone Policy‑Verbesserung.\n",
    "\n",
    "Merksatz\n",
    "- TRPO ist NPG mit Sicherheitsgurt: große Schritte sind erlaubt, aber nur innerhalb einer vertrauenswürdigen Region.\n",
    "\n",
    "\n",
    "### 6. PPO – Proximal Policy Optimization\n",
    "„Fast so stabil wie TRPO, aber viel einfacher und schneller.“\n",
    "\n",
    "Kernidee\n",
    "- PPO approximiert die TRPO‑Idee, aber ohne komplizierte Constraints.\n",
    "- Stattdessen nutzt PPO eine Clipping‑Funktion, die Updates begrenzt, wenn die Policy zu stark abweicht.\n",
    "\n",
    "Wesentliche Eigenschaften\n",
    "- Einfach zu implementieren, sehr robust.\n",
    "- Clipped Objective verhindert destruktive Updates.\n",
    "- Funktioniert hervorragend in der Praxis, Standard‑Baseline für viele RL‑Tasks.\n",
    "- On‑policy, aber mit Mini‑Batch‑SGD → effizienter als klassische PG‑Methoden.\n",
    "\n",
    "Merksatz\n",
    "- PPO ist TRPO ohne Kopfschmerzen: stabil, performant und leicht zu trainieren.\n",
    "\n",
    "| Methode | On/Off‑Policy | Stabilität | Varianz | Besonderheit | Kurzbeschreibung |\n",
    "|--------|---------------|------------|---------|--------------|------------------|\n",
    "| **VPG** | On‑policy | Niedrig | Hoch | Einfachster Policy‑Gradient | Roher Gradient ohne Stabilisierung |\n",
    "| **NPG** | On‑policy | Mittel | Niedriger | Natural Gradient | Berücksichtigt die Geometrie des Policy‑Raums |\n",
    "| **A2C** | On‑policy | Mittel | Niedrig | Actor‑Critic | Critic reduziert Varianz der Gradienten |\n",
    "| **TRPO** | On‑policy | Hoch | Niedrig | KL‑Constraint, Trust Region | Sichere Updates durch begrenzte Policy‑Änderung |\n",
    "| **PPO** | On‑policy | Hoch | Niedrig | Clipped Objective | TRPO‑Idee, aber einfach und effizient |\n",
    "| **SAC** | Off‑policy | Sehr hoch | Sehr niedrig | Entropiebonus, Replay Buffer, Double‑Q | Moderne, stabile, explorative Continuous‑Control‑Methode |\n",
    "\n",
    "\n",
    "## Entwicklungslinie (Evolution):\n",
    "Diese Reihenfolge zeigt, wie sich die Methoden logisch aus Problemen und Verbesserungen der Vorgänger ergeben.\n",
    "\n",
    "- VPG – Wir machen rohe Gradientenupdates.\n",
    "- NPG – Wir berücksichtigen die Geometrie.\n",
    "- A2C – Wir reduzieren Varianz durch einen Critic.\n",
    "- TRPO – Wir machen sichere Updates mit KL‑Constraint.\n",
    "- PPO – Wir vereinfachen TRPO und machen es praktisch.\n",
    "- SAC – Wir gehen Off‑Policy, nutzen Entropie und Replay Buffer.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
