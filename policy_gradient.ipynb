{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f221148d",
   "metadata": {},
   "source": [
    "# Policy‑Gradient‑Learning\n",
    "- lässt sich am besten verstehen, wenn man sich klar macht, was ein Agent eigentlich lernt: nicht eine Wertfunktion, sondern direkt eine Strategie.\n",
    "\n",
    "## Grundidee\n",
    "Beim Policy‑Gradient‑Verfahren wird die Policy selbst optimiert, also eine Funktion\n",
    "\\pi _{\\theta }(a\\mid s), die für jeden Zustand s eine Wahrscheinlichkeitsverteilung über Aktionen a liefert.\n",
    "Die Parameter \\theta  (z. B. Gewichte eines neuronalen Netzes) werden so angepasst, dass Aktionen, die langfristig zu hohen Rewards führen, wahrscheinlicher werden.\n",
    "Das ist der Kern:\n",
    "Erhöhe die Wahrscheinlichkeit guter Aktionen, senke die Wahrscheinlichkeit schlechter Aktionen.\n",
    "\n",
    "## Warum überhaupt Policy Gradients?\n",
    "Viele RL‑Methoden (z. B. Q‑Learning) lernen eine Wertfunktion und leiten daraus eine Policy ab.\n",
    "Das funktioniert gut, aber:\n",
    "- Stochastische Policies sind schwer über Q‑Funktionen zu optimieren.\n",
    "- In kontinuierlichen Aktionsräumen ist Q‑Learning oft unpraktisch.\n",
    "- Policy‑Gradient‑Methoden sind direkt differenzierbar und elegant.\n",
    "\n",
    "## Mathematische Form\n",
    "Das Ziel ist, den erwarteten Return zu maximieren:\n",
    "$$J(\\theta )=\\mathbb{E_{\\mathnormal{\\pi _{\\theta }}}}[R]$$\n",
    "Der Policy‑Gradient‑Satz liefert:\n",
    "\n",
    "Das ist der berühmte REINFORCE‑Gradient.\n",
    "Interpretation:\n",
    "\n",
    "$$\\log \\pi _{\\theta }(a\\mid s)$$\n",
    "\n",
    "- sagt, wie stark die Parameter die Aktion beeinflussen.\n",
    "- R verstärkt oder schwächt diese Richtung.\n",
    "\n",
    "## Intuition\n",
    "Stell dir vor, der Agent probiert Aktionen aus.\n",
    "Wenn eine Aktion zu hohem Reward führt:\n",
    "- Der Term R ist groß\n",
    "- Der Gradient zeigt in Richtung „mach diese Aktion häufiger“\n",
    "Wenn der Reward schlecht ist:\n",
    "- Der Term R ist klein oder negativ\n",
    "- Die Policy wird von dieser Aktion wegbewegt\n",
    "Das ist stochastischer Gradientenaufstieg auf der Policy.\n",
    "\n",
    "## Praktische Varianten\n",
    "Policy‑Gradient‑Methoden sind die Basis vieler moderner RL‑Algorithmen:\n",
    "| Methode      | Idee                                           |\n",
    "|--------------|------------------------------------------------|\n",
    "| REINFORCE    | Einfacher Monte‑Carlo‑Policy‑Gradient          |\n",
    "| Actor‑Critic | Policy‑Gradient + Wertfunktion als Baseline    |\n",
    "| A2C / A3C    | Parallelisierte Actor‑Critic‑Varianten         |\n",
    "| PPO          | Stabilisiert Updates durch Clipping            |\n",
    "| TRPO         | Optimiert Policy unter KL‑Constraint           |\n",
    "\n",
    "## Warum Baselines wichtig sind\n",
    "REINFORCE hat hohe Varianz.\n",
    "Deshalb nutzt man oft eine Baseline b(s), typischerweise eine Wertfunktion V(s):\n",
    "\n",
    "Das ändert den Erwartungswert nicht, reduziert aber die Varianz massiv.\n",
    "\n",
    "## Kurzfassung\n",
    "Policy‑Gradient‑Learning bedeutet:\n",
    "- Direkte Optimierung der Policy\n",
    "- Gradientenaufstieg auf den erwarteten Return\n",
    "- Stochastische Policies werden natürlich unterstützt\n",
    "- Sehr gut für kontinuierliche Aktionsräume\n",
    "- Grundlage moderner RL‑Algorithmen wie PPO, TRPO, A2C\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
