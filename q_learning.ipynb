{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91063786",
   "metadata": {},
   "source": [
    "# Q-Learning\n",
    "1. Grundidee: Q‑Learning mit Funktionsapproximation\n",
    "Q‑Learning aktualisiert Werte nach: \n",
    "\n",
    "$$Q(s,a)\\leftarrow r+\\gamma \\max _{a'}Q(s',a')$$\n",
    "\n",
    "Bei großen oder kontinuierlichen Zustandsräumen ist eine Tabelle unmöglich → ein Neural Network approximiert Q.\n",
    "\n",
    "2. Das Q‑Netzwerk\n",
    "Das Netzwerk bekommt den Zustand s als Input und gibt für jede Aktion einen Q‑Wert aus:\n",
    "- Input: Zustand (z. B. Position, Geschwindigkeit)\n",
    "- Output: Vektor der Q‑Werte für alle Aktionen\n",
    "Beispiel CartPole:\n",
    "Input: 4 Werte → Output: 2 Q‑Werte (left, right)\n",
    "\n",
    "3. Experience Replay (Replay Buffer)\n",
    "Statt jede Transition sofort zu lernen, speichert DQN sie in einem Replay Buffer:\n",
    "$$(s,a,r,s',\\mathrm{done})$$\n",
    "Beim Training werden zufällige Batches gezogen.\n",
    "Vorteile:\n",
    "- Bricht Korrelationen zwischen Samples\n",
    "- Stabilisiert das Lernen\n",
    "- Erhöht Sample‑Effizienz\n",
    "Das ist einer der entscheidenden Tricks, warum DQN funktioniert.\n",
    "\n",
    "4. Target Network\n",
    "Ein zweites Netzwerk, das langsam aktualisiert wird, liefert stabile Zielwerte:\n",
    "$$y=r+\\gamma \\max _{a'}Q_{\\mathrm{target}}(s',a')$$\n",
    "Warum?\n",
    "- Verhindert, dass das Netz seine eigenen Ziele ständig verschiebt\n",
    "- Reduziert Divergenz\n",
    "Das Target‑Netz wird alle N Schritte kopiert oder „soft updated“.\n",
    "\n",
    "5. Loss‑Funktion\n",
    "Das Training minimiert den TD‑Error:\n",
    "$$L=\\left( Q_{\\mathrm{online}}(s,a)-y\\right) ^2$$\n",
    "Das ist ein klassischer MSE‑Loss.\n",
    "\n",
    "6. Exploration: ε‑Greedy\n",
    "Während des Trainings:\n",
    "- Mit Wahrscheinlichkeit ε → zufällige Aktion\n",
    "- Mit Wahrscheinlichkeit 1−ε → beste Aktion laut Q‑Netz\n",
    "ε wird typischerweise über Zeit reduziert (ε‑Decay).\n",
    "\n",
    "7. Trainingsschleife (kompakt)\n",
    "Eine Episode läuft so:\n",
    "- Zustand s beobachten\n",
    "- Aktion a wählen (ε‑greedy)\n",
    "- Schritt ausführen → r,s'\n",
    "- Transition in Replay Buffer speichern\n",
    "- Mini‑Batch aus Replay Buffer ziehen\n",
    "- Zielwert y mit Target‑Netz berechnen\n",
    "- Online‑Netz trainieren\n",
    "- Target‑Netz periodisch aktualisieren\n",
    "Das wiederholt sich über viele Episoden.\n",
    "\n",
    "Warum funktioniert DQN so gut?\n",
    "\n",
    "Weil es drei Probleme löst, die klassisches Q‑Learning mit NN instabil machen:\n",
    "\n",
    "\n",
    "![alt text](q_learning.png)\n",
    "\n",
    "\n",
    "Diese drei Mechanismen machen DQN zu einem robusten RL‑Algorithmus.\n",
    "\n",
    "Beispiel: Was macht das Netz eigentlich?\n",
    "Wenn das Netz lernt, dass „Pole fällt nach rechts“, dann steigt der Q‑Wert für Aktion „left“.\n",
    "Das Netz lernt also Wertschätzungen, nicht direkt Aktionen.\n",
    "\n",
    "Kurzfassung in einem Satz\n",
    "Ein DQN approximiert die Q‑Funktion mit einem neuronalen Netz, stabilisiert das Lernen durch Replay Buffer und Target‑Netzwerke und wählt Aktionen über ε‑Greedy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d53e8a3",
   "metadata": {},
   "source": [
    "# Verbesserungen\n",
    "\n",
    "1. Verbesserungen am Q‑Learning‑Update\n",
    "Double DQN (DDQN)\n",
    "- Trennt Aktionswahl und Aktionsbewertung\n",
    "- Reduziert Overestimation Bias\n",
    "- Sehr einfache Erweiterung, großer Effekt\n",
    "\n",
    "Dueling DQN\n",
    "- Zerlegt Q(s,a) in:\n",
    "- V(s) = Wert des Zustands\n",
    "- A(s,a) = Vorteil der Aktion\n",
    "- Hilft besonders in Umgebungen, in denen viele Aktionen ähnlich gut sind\n",
    "- Stabilisiert das Lernen\n",
    "\n",
    "Averaged-DQN\n",
    "- Nutzt mehrere Q‑Netze und mittelt deren Schätzungen\n",
    "- Reduziert Varianz und Overestimation\n",
    "- Teurer, aber robuster\n",
    "\n",
    "Maxmin DQN\n",
    "- Mehrere Q‑Netze, aber nimmt das Minimum\n",
    "- Noch stärkere Kontrolle von Overestimation\n",
    "\n",
    "2. Verbesserungen am Replay Buffer\n",
    "Prioritized Experience Replay (PER)\n",
    "- Wählt wichtige Transitionen häufiger\n",
    "- Beschleunigt Lernen deutlich\n",
    "- Nutzt TD‑Error als Priorität\n",
    "\n",
    "N‑Step Replay\n",
    "- Nutzt n‑Schritt‑Returns statt 1‑Step\n",
    "- Bessere Credit Assignment\n",
    "- Häufig kombiniert mit PER\n",
    "\n",
    "Replay Buffer mit Segmentierung / Reservoir Sampling\n",
    "- Für Non‑stationäre Umgebungen\n",
    "- Verhindert, dass alte Daten zu dominant werden\n",
    "\n",
    "3. Verbesserungen an der Exploration\n",
    "Noisy DQN\n",
    "- Ersetzt ε‑Greedy durch stochastische, lernbare Rauschschichten\n",
    "- Exploration wird Teil des Modells\n",
    "- Sehr elegant und oft besser als ε‑Greedy\n",
    "\n",
    "Bootstrapped DQN\n",
    "- Mehrere Q‑Heads, zufällige Maskierung\n",
    "- Liefert Thompson Sampling‑ähnliche Exploration\n",
    "- Sehr stark in komplexen Umgebungen\n",
    "\n",
    "4. Verbesserungen an der Zielwertschätzung\n",
    "Distributional DQN (C51)\n",
    "- Lernt nicht nur den Erwartungswert Q(s,a), sondern die ganze Verteilung\n",
    "- Führt zu stabilerem und oft besserem Verhalten\n",
    "- Grundlage für viele moderne Agenten\n",
    "\n",
    "Quantile Regression DQN (QR‑DQN)\n",
    "- Approximiert die Verteilung über Quantile\n",
    "- Flexibler als C51\n",
    "- Grundlage für IQN\n",
    "\n",
    "Implicit Quantile Networks (IQN)\n",
    "- Kontinuierliche Quantile\n",
    "- Sehr leistungsfähig\n",
    "- State‑of‑the‑art in vielen Atari‑Benchmarks\n",
    "\n",
    "5. Kombinationen der besten Ideen\n",
    "Rainbow DQN\n",
    "Die „All‑Star‑Version“ von DQN.\n",
    "Kombiniert:\n",
    "- Double DQN\n",
    "- Dueling\n",
    "- Prioritized Replay\n",
    "- Multi‑Step Learning\n",
    "- Distributional RL (C51)\n",
    "- Noisy Nets\n",
    "- (teilweise) L2‑Regularisierung\n",
    "Rainbow ist heute der De‑facto‑Standard für DQN‑basierte Agenten.\n",
    "\n",
    "6. Erweiterungen für spezielle Szenarien\n",
    "Recurrent DQN (DRQN)\n",
    "- Nutzt LSTM/GRU\n",
    "- Für POMDPs (teilweise beobachtbare Umgebungen)\n",
    "- Z. B. bei visuellen RL‑Tasks mit Flickering Frames\n",
    "\n",
    "Parametric DQN\n",
    "- Für Umgebungen mit variablen Aktionsräumen\n",
    "- Z. B. Empfehlungssysteme, Dialogsysteme\n",
    "\n",
    "Multi‑Agent DQN‑Varianten\n",
    "- VDN, QMIX (nicht direkt DQN, aber Q‑Learning‑basiert)\n",
    "- Für kooperative Multi‑Agent‑Settings\n",
    "\n",
    "Eine sinnvolle Progression ist:\n",
    "- DQN\n",
    "- Double DQN\n",
    "- Dueling + PER\n",
    "- Noisy Nets\n",
    "- Distributional (C51 oder QR‑DQN)\n",
    "- Rainbow\n",
    "- Bootstrapped / IQN, je nach Forschungsinteresse"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
