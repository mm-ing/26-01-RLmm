 1. Klassische Lowâ€‘Dimensional Environments (VektorzustÃ¤nde)
CartPole, MountainCar, Acrobot, LunarLander
Diese Umgebungen haben kleine ZustandsrÃ¤ume und diskrete Aktionen. Hier geht es vor allem um StabilitÃ¤t, Exploration und Overestimationâ€‘Kontrolle.
CartPole-v1
- Beste Varianten:
- Double DQN
- Dueling DQN
- PER
- Warum:
- Overestimation ist ein Thema â†’ DDQN
- Viele Aktionen sind Ã¤hnlich â†’ Dueling
- PER beschleunigt das Lernen
- Overkill: Distributional, Noisy Nets, Rainbow (funktioniert, aber unnÃ¶tig)

MountainCar-v0
- Beste Varianten:
- PER
- Nâ€‘Step DQN
- Double DQN
- Warum:
- Reward ist extrem spÃ¤rlich â†’ PER + Nâ€‘Step helfen massiv
- DDQN stabilisiert die Qâ€‘SchÃ¤tzung
- Optional: Noisy Nets fÃ¼r bessere Exploration

Acrobot-v1
- Beste Varianten:
- Double DQN
- Dueling DQN
- PER
- Warum:
- Hohe Dynamik, viele suboptimale Aktionen â†’ Dueling
- PER beschleunigt das Finden guter Trajektorien

LunarLander-v2
- Beste Varianten:
- Double DQN
- Dueling DQN
- PER
- Noisy DQN
- Warum:
- Reward ist nicht spÃ¤rlich, aber noisy â†’ Noisy Nets helfen
- Dueling + DDQN sind fast Pflicht
- Sehr gut: Rainbow (hier lohnt es sich)

ðŸ”µ 2. Pixelâ€‘basierte Environments (Atari, MinAtar, Retro)
Breakout, Pong, Space Invaders, etc.
Hier brauchst du starke Featureâ€‘Extraktion + stabile Qâ€‘SchÃ¤tzung.
Atari (ALE)
- Beste Varianten:
- Rainbow DQN (State of the Art)
- Distributional DQN (C51, QRâ€‘DQN, IQN)
- Noisy Nets
- Nâ€‘Step
- Double + Dueling
- Warum:
- Pixelinput â†’ CNN + Distributional RL ist extrem stark
- Exploration ist schwierig â†’ Noisy Nets oder Bootstrapped DQN
- Multiâ€‘Step verbessert Credit Assignment
Kurz: FÃ¼r Atari ist Rainbow die Benchmark.

MinAtar
- Beste Varianten:
- Distributional DQN
- Noisy Nets
- PER
- Warum:
- Weniger komplex als Atari, aber gleiche Strukturen
- Distributional RL bringt hier besonders viel

ðŸŸ  3. Stochastische oder teilweise beobachtbare Umgebungen
Flickering Atari, POMDPâ€‘Varianten, Env mit Masking
Flickering Atari / POMDPâ€‘Varianten
- Beste Varianten:
- DRQN (Recurrent DQN)
- Bootstrapped DQN
- Noisy Nets
- Warum:
- LSTM/GRU kompensiert fehlende Beobachtungen
- Bootstrapped DQN liefert bessere Exploration bei Unsicherheit

ðŸŸ£ 4. Multiâ€‘Agentâ€‘Environments (z.â€¯B. PettingZoo mit diskreten Aktionen)
Kooperative Settings
- Beste Varianten:
- VDN / QMIX (Qâ€‘Learningâ€‘basiert, aber nicht klassisch DQN)
- Double DQN als Basis fÃ¼r einzelne Agenten
- Warum:
- Jointâ€‘Actionâ€‘Spaces explodieren â†’ Faktorisierung nÃ¶tig

ðŸŸ¤ 5. Environments mit sehr vielen diskreten Aktionen
z.â€¯B. Empfehlungssystemâ€‘Ã¤hnliche Gymâ€‘Envs
Parametric DQN
- Beste Varianten:
- Parametric DQN
- Dueling + PER
- Warum:
- Klassisches DQN skaliert schlecht bei 100+ Aktionen
- Parametric DQN modelliert Aktionen als Features

ðŸŸ¥ 6. Nicht geeignet fÃ¼r DQN (aber oft gefragt)
Pendulum, Continuous Control, MuJoCo
â†’ Kein DQNâ€‘Variant geeignet, da kontinuierliche Aktionen.
â†’ Nutze SAC, TD3, DDPG.

Empfohlene Pipeline je nach KomplexitÃ¤t

![alt text](gym_complex_pipeline.png)
